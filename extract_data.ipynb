{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99936540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b121bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the excel sheet and skip blank rows\n",
    "raw_excel = pd.read_excel(\"data/DSHA LIHTC List_MAPPING.xlsx\", engine='openpyxl', skiprows=[2,3], skipfooter=4, dtype=str)\n",
    "# Add additional column information from the first row\n",
    "raw_excel.columns = (raw_excel.columns.astype(str) + \" \" + raw_excel.head(1).fillna(\"\").astype(str)).iloc[0].str.strip().values\n",
    "raw_excel.rename(columns={\"ALLOCATION .1 DATE\": \"ALLOCATION DATE\", \"ALLOCATION  AMOUNT\": \"ALLOCATION AMOUNT\", \"Type of Property*\": \"Type of Property\"}, inplace=True)\n",
    "raw_excel.drop(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade04c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and label the rows about tax year\n",
    "raw_excel[\"is tax\"] = raw_excel[\"PROJECT NAME & ADDRESS\"].str.contains(\"TAX CREDIT ALLOCATIONS\")\n",
    "\n",
    "# Function that maps each the boolean column \"is tax\", which is true when a row contains tax year information, to an integer equal to the tax year\n",
    "def assign_to_year(x, i):\n",
    "    # if the row is a tax year, increment i\n",
    "    if x:\n",
    "        i[0] = i[0] + 1\n",
    "    # return an integer equal to the tax year\n",
    "    return i[0] + 2016\n",
    "\n",
    "# add a column for the tax year\n",
    "index = [-1]\n",
    "raw_excel[\"Tax Allocation Year\"] = raw_excel[\"is tax\"].apply(assign_to_year, args=[index])\n",
    "\n",
    "# drop rows of tax year information and reformat\n",
    "raw_excel = raw_excel.loc[~raw_excel[\"is tax\"]].drop(columns=\"is tax\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8069ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column that labels the primary line for an entry\n",
    "raw_excel[\"primary\"] = ~raw_excel[\"County\"].isna()\n",
    "\n",
    "# Function that maps each the boolean column \"primary\", which is true when a row contains the primary info from the dataset, to an integer that functions as an index for primary entries\n",
    "def assign_to_year(x, i):\n",
    "    # if the row is primary, increment i\n",
    "    if x:\n",
    "        i[0] = i[0] + 1\n",
    "    # return an index for the primary entries\n",
    "    return i[0]\n",
    "\n",
    "# add an index column for the primary entries\n",
    "index = [-1]\n",
    "raw_excel[\"primary\"] = raw_excel[\"primary\"].apply(assign_to_year, args=[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de59457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate dataframes for each row in a data entry\n",
    "grouped_data = raw_excel.groupby(\"primary\")\n",
    "raw_data1 = grouped_data.nth(0)\n",
    "raw_data2 = grouped_data.nth(1).drop(columns=\"Tax Allocation Year\")\n",
    "raw_data3 = grouped_data.nth(2).drop(columns=\"Tax Allocation Year\")\n",
    "raw_data4 = grouped_data.nth(3).drop(columns=\"Tax Allocation Year\")\n",
    "raw_data5 = grouped_data.nth(4).drop(columns=\"Tax Allocation Year\")\n",
    "\n",
    "# Modify the column names for each dataframe to prepare for joining\n",
    "raw_data2.columns = raw_data2.columns + \" 2\"\n",
    "raw_data3.columns = raw_data3.columns + \" 3\"\n",
    "raw_data4.columns = raw_data4.columns + \" 4\"\n",
    "raw_data5.columns = raw_data5.columns + \" 5\"\n",
    "\n",
    "# Join the dataframes by index\n",
    "flattened_data = raw_data1.join(raw_data2, how=\"left\").join(raw_data3, how=\"left\").join(raw_data4, how=\"left\").join(raw_data5, how=\"left\").dropna(axis=1, how='all').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ff1246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates back to their orginial format\n",
    "flattened_data[\"Placed in Service Date\"] = pd.to_datetime(flattened_data[\"Placed in Service Date\"], errors='coerce').dt.strftime('%m/%d/%Y')\n",
    "flattened_data[\"ALLOCATION DATE\"] = pd.to_datetime(flattened_data[\"ALLOCATION DATE\"], errors='coerce').dt.strftime('%m/%d/%Y')\n",
    "flattened_data[\"Tax Credit Compliance Date\"] = pd.to_datetime(flattened_data[\"Tax Credit Compliance Date\"], errors='coerce').dt.strftime('%m/%d/%Y')\n",
    "flattened_data[\"Extended Use Period\"] = pd.to_datetime(flattened_data[\"Extended Use Period\"], errors='coerce').dt.strftime('%m/%d/%Y')\n",
    "flattened_data[\"Placed in Service Date 2\"] = pd.to_datetime(flattened_data[\"Placed in Service Date 2\"], errors='coerce').dt.strftime('%m/%d/%Y')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82d1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine address fields\n",
    "address_columns = [\"PROJECT NAME & ADDRESS\", \"PROJECT NAME & ADDRESS 2\", \"PROJECT NAME & ADDRESS 3\", \"PROJECT NAME & ADDRESS 4\", \"PROJECT NAME & ADDRESS 5\"]\n",
    "\n",
    "# Extracts addresses from projects\n",
    "def extract_address(x):\n",
    "    # Project 27 has three full addresses, so we use the last one listed\n",
    "    if x.name == 27:\n",
    "        addr = x[address_columns].dropna().values[-1]\n",
    "        return addr\n",
    "    # The last two lines of the address field contain the address split between two lines, except for project 27\n",
    "    else:\n",
    "        addr = x[address_columns].dropna().values[-2:]\n",
    "        return addr[0] + \", \" + addr[1]\n",
    "\n",
    "# Extract an address for each project\n",
    "flattened_data[\"address\"] = flattened_data.apply(extract_address, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd04535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the data to a csv\n",
    "flattened_data.to_csv(\"data/processed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756363da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the addresses to a seperate list\n",
    "flattened_data[\"address\"].to_csv(\"data/DSHA_addresses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba495db",
   "metadata": {},
   "source": [
    "At this point we transfer the address csv over to the geolocator to get the latitude and longitude of each project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a62066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file of geolocated addresses\n",
    "geolocations = pd.read_csv(\"data/counts_per_tract.csv\")\n",
    "# Join the geolocations to the flattened dataframe\n",
    "geolocated_data = flattened_data.merge(geolocations, left_on=\"address\", right_on=\"input addresses\")\n",
    "# Remove lat,lot from unsuccessfully (not in the u.s.) geolocated address\n",
    "geolocated_data.loc[geolocated_data[\"census tract\"] == \"Unable To Geolocate The Address\", \"lot\"] = np.nan\n",
    "geolocated_data.loc[geolocated_data[\"census tract\"] == \"Unable To Geolocate The Address\", \"lat\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc3e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lat,lot to Shapely points\n",
    "geolocated_data = gpd.GeoDataFrame(geolocated_data, geometry=gpd.points_from_xy(geolocated_data['lat'], geolocated_data['lot'], crs=\"EPSG:4326\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415df177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize points on a map\n",
    "\n",
    "# initialize the map and store it in a folium map object\n",
    "us_map = folium.Map(location=[39.74503, -75.57203], zoom_start=14, tiles=None)\n",
    "\n",
    "# Add background tiles\n",
    "folium.TileLayer('CartoDB positron',name=\"Light Map\",control=False).add_to(us_map)\n",
    "\n",
    "\n",
    "# Add markers for each school\n",
    "points=folium.features.GeoJson(\n",
    "        geolocated_data.loc[geolocated_data[\"census tract\"] != \"Unable To Geolocate The Address\"], # Full geopandas data\n",
    "        control=False,\n",
    "        marker = folium.CircleMarker(radius = 5, # Radius in metres\n",
    "                           weight = 0, #outline weight\n",
    "                           fill_color = '#d95f02', \n",
    "                           fill_opacity = 1)\n",
    "        )\n",
    "\n",
    "points.add_to(us_map)\n",
    "us_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract shape files for senate districts\n",
    "senate_districts = gpd.read_file(\"data/Enacted-Senate-EsriShp (1).zip\")\n",
    "\n",
    "# Gets the senate district containing a point\n",
    "def get_district(x):\n",
    "    # Return a blank when an address could not be geolocated\n",
    "    if x[\"census tract\"] == \"Unable To Geolocate The Address\":\n",
    "        return \"\"\n",
    "    # Return the senate district containing the point otherwise\n",
    "    else:\n",
    "        return senate_districts.loc[x[\"geometry\"].within(senate_districts[\"geometry\"])][\"DISTRICT\"].values[0]\n",
    "\n",
    "# Add a column for senate district\n",
    "geolocated_data[\"Senate District\"] = geolocated_data.apply(get_district, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245cc71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocated_data.drop(columns=[\"address\", \"input addresses\", \"census tract\", \"lot\", \"lat\"]).to_file(\"data/DSHA_districted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d2f323",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocated_data.drop(columns=[\"address\", \"input addresses\", \"census tract\", \"lot\", \"lat\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3089510f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
